---
title: "Atelier pour d√©couvrir la r√©cup√©ration de donn√©es via avec le format Parquet"
author: Lino Galiana
date: 2025-04-09
description: |
  Le format `Parquet` est un format de donn√©es connaissant une popularit√© importante du fait de ses caract√©ristiques techniques (orientation colonne, compression, interop√©rabilit√©...), de sa nature _open source_ et du riche √©cosyst√®me associ√© dont les frameworks les plus pro√©minents sont `Arrow` et `DuckDB`. A ces nombreux avantages s'ajoutent une int√©gration native aux infrastructures _cloud_ bas√©es sur `S3`, des extensions nombreuses pour traiter des donn√©es complexes comme les donn√©es g√©ographiques ou, plus r√©cemment, le portage en WASM de `DuckDB` permettant de construire des applications r√©actives impliquant des transformations de donn√©es directement depuis le navigateur. 
number-sections: true
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/serveurpython.jpg
---



<a href="https://datalab.sspcloud.fr/launcher/ide/rstudio?name=Atelier%20Parquet%20SSPHub&shared=false&version=2.2.6&persistence.size=¬´19Gi¬ª&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2FSSPHub%2Fssphub-ateliers-parquet%2Frefs%2Fheads%2Fmain%2Finit_r.sh¬ª&networking.user.enabled=true&autoLaunch=true" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSP%20Cloud-Ouvrir_dans_RStudio-blue?logo=r&amp;logoColor=blue" alt="Onyxia"></a>
<a href="https://github.com/InseeFrLab/ssphub-ateliers/tree/main/R" target="_blank" rel="noopener" data-original-href="https://github.com/InseeFrLab/ssphub-ateliers/tree/main/R"><img src="https://custom-icon-badges.demolab.com/badge/download--r--script-black.svg?logo=download-cloud&logoSource=feather" alt="Onyxia"></a><br>
<a href="https://datalab.sspcloud.fr/launcher/ide/vscode-python?name=SSPHub-Atelier-Parquet&shared=false&version=2.2.12&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2FSSPHub%2Fssphub-ateliers-parquet%2Frefs%2Fheads%2Fmain%2Finit_python.sh¬ª&networking.user.enabled=true&autoLaunch=true" target="_blank" rel="noopener" data-original-href="https://datalab.sspcloud.fr/launcher/ide/vscode-python?name=SSPHub-Atelier-Parquet&shared=false&version=2.2.12&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2FSSPHub%2Fssphub-ateliers-parquet%2Frefs%2Fheads%2Fmain%2Finit_python.sh¬ª&networking.user.enabled=true&autoLaunch=true"><img src="https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&amp;logoColor=white" alt="Onyxia"></a>
<a href="https://github.com/SSPHub/ssphub-ateliers-parquet/blob/main/python/_s3.qmd" target="_blank" rel="noopener" data-original-href="https://github.com/SSPHub/ssphub-ateliers-parquet/blob/main/python/_s3.qmdb"><img src="https://custom-icon-badges.demolab.com/badge/download--notebook-black.svg?logo=download-cloud&logoSource=feather" alt="Onyxia"></a><br>


<details open>

<summary>

Afficher les _slides_ associ√©es

</summary>

<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://inseefrlab.github.io/ssphub-ateliers-slides/slides-data/parquet"></iframe></div>


_[Cliquer ici](https://inseefrlab.github.io/ssphub-ateliers-slides/slides-data/parquet){target="_blank"}
pour les afficher en plein √©cran._

</details>


<details>


<summary>

Regarder le _replay_ de la session _live_ du 09 Avril 2025:

</summary>

{{< video https://minio.lab.sspcloud.fr/lgaliana/ssphub/replay/20250416_masterclass_parquet/GMT20250416-130715_Recording_1686x768.mp4 >}}


</details>


# Introduction

Tout au long de ce tutoriel guid√©, nous allons voir comment utiliser le format `Parquet` de mani√®re la plus efficiente.

Afin de comparer les diff√©rents formats et m√©thodes d'utilisation, nous allons g√©n√©ralement **comparer le temps d'ex√©cution et l'usage m√©moire d'une requ√™te standard**.


## Etapes pr√©liminaires

Au cours de cet atelier, nous aurons besoin des _packages_ suivants:

```{.r}
#| output: false
library(duckdb)
library(glue)
library(DBI)
library(dplyr)
library(dbplyr)
library(mapview)
```


Ce tutoriel s'appuie sur des donn√©es ouvertes diffus√©es au format `Parquet`. Pour les r√©cup√©rer, vous pouvez ex√©cuter le script suivant:

<details>

<summary>
R√©cup√©rer les donn√©es
</summary>

```{.r include="./R/create_environment.R"}
```

</details>

# Passer de `CSV` √† `Parquet`

Commen√ßons par comparer les formats `CSV` et `Parquet` afin de comprendre les gains qu'apporte d√©j√† ce format. 

Le prochain chapitre propose d'utiliser le _package_ [`bench`](https://bench.r-lib.org/) pour les comparatifs. Il est plus simple d'encapsuler dans ces _benchmarks_ des fonctions: vous pouvez d√©velopper le code puis l'int√©grer dans une fonction _ad hoc_. 

Pour ce premier exercice, nous proposons d'utiliser `Arrow` pour la lecture des fichiers. Nous verrons ult√©rieurement comment faire la m√™me chose avec `DuckDB`.

:::{.exercise}
## Exercice 1 : Du `CSV` au `Parquet` {.unnumbered}

* La requ√™te suivante permet de calculer les donn√©es pour construire une pyramide des √¢ges sur un d√©partement donn√©, √† partir du fichier `CSV` du recensement. Apr√®s l'avoir test√©e, encapsuler celle-ci dans une fonction `req_csv` (sans argument).

```{.r}
res <- readr::read_csv("data/RPindividus_24.csv") |>
    filter(DEPT == "36") |>
    group_by(AGED, DEPT) |>
    summarise(n_indiv = sum(IPONDI))
```

* Sur le m√™me mod√®le, construire une fonction `req_read_parquet` bas√©e cette fois sur le fichier `data/RPindividus_24.parquet` charg√© avec la fonction [read_parquet](https://arrow.apache.org/docs/r/reference/read_parquet.html) d'`Arrow`

* Comparer les performances (temps d'ex√©cution et allocation m√©moire) de ces deux m√©thodes gr√¢ce √† la fonction [bench::mark](https://bench.r-lib.org/#benchmark), √† laquelle on passera les param√®tres `iterations = 1` (comparaison √† partir d'une seule it√©ration) et `check = FALSE` (autorise les outputs des deux fonctions √† √™tre diff√©rents).

* Arranger les donn√©es pour avoir un tableau de r√©sultats
:::

<details>

<summary>
Proposition de correction
</summary>

```{.r include="./R/benchmark_functions.R"}
```

```{.r include="./R/exo1.R"}
```


</details>

<script>
  function resizeIframe(obj) {
    obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
  }
</script>

<iframe src="./bench/mark1.html" style="width: 100%; border-style: none; height: 0; overflow: hidden"
        onload="resizeIframe(this)"></iframe>


# Comprendre l'int√©r√™t de la _lazy evaluation_

La partie pr√©c√©dente a montr√© un **gain de temps consid√©rable** du passage de `CSV` √† `Parquet`. N√©anmoins, l'**utilisation m√©moire √©tait encore tr√®s √©lev√©e** alors qu'on utilise de fait qu'une infime partie du fichier. 

Dans cette partie, on va voir comment utiliser la ***lazy evaluation*** et les **optimisations du plan d'ex√©cution** effectu√©es par `Arrow` pour exploiter pleinement la puissance du format `Parquet`.

## La _lazy evaluation_ en pratique

::: {.exercise}
## Exercice 2 : Exploiter la *lazy evaluation* et les optimisations d'`Arrow` {.unnumbered}

* Utiliser la fonction [arrow::open_dataset](https://arrow.apache.org/docs/r/reference/open_dataset.html) pour ouvrir le fichier `data/RPindividus_24.parquet`. Regarder la classe de l'objet obtenu.
* Afficher les 5 premi√®res lignes de la table avec la fonction `head()`. Observer l'objet obtenu (sortie en console, classe).
* Faire la m√™me chose avec `duckdb` (par le biais de l'API _tidyverse_ et en SQL direct)

* Prenez ce code:

```{.r}
arrow::open_dataset(path_parquet_subset) |>
    filter(DEPT == "36") |>
    group_by(AGED, DEPT) |>
    summarise(n_indiv = sum(IPONDI))
```

et ex√©cutez le. Que se passe-t-il √† votre avis ?

* Ajouter une √©tape `collect()` √† la fin de cette cha√Æne. Comprenez-vous la diff√©rence ?

* Construire une fonction `req_open_dataset` sur le mod√®le de celles de la partie pr√©c√©dente, qui importe cette fois les donn√©es avec la fonction [arrow::open_dataset](https://arrow.apache.org/docs/r/reference/open_dataset.html)
* Comparer les performances (temps d'ex√©cution et allocation m√©moire) de la m√©thode `read_parquet` et de la m√©thode `open_dataset` gr√¢ce √† la fonction [bench::mark](https://bench.r-lib.org/#benchmark)


:::

<iframe src="./bench/mark2.html" style="width: 100%; border-style: none; height: 0; overflow: hidden"
        onload="resizeIframe(this)"></iframe>


## Comprendre l'optimisation permise par `Parquet` et `DuckDB`

Pour r√©duire la volum√©trie des donn√©es import√©es, il est possible de mettre en oeuvre deux strat√©gies:

- N'importer qu'un nombre limit√© de colonnes
- N'importer qu'un nombre limit√© de lignes

Comme cela a √©t√© √©voqu√© dans les _slides_, le format `Parquet` est particuli√®rement optimis√© pour le premier besoin. C'est donc g√©n√©ralement la premi√®re optimisation mise en oeuvre. Pour s'en convaincre on peut regarder la taille des donn√©es import√©es dans deux cas:

- On utilise beaucoup de lignes mais peu de colonnes
- On utilise beaucoup de colonnes mais peu de lignes

Pour cela, nous utilisons la fonction SQL `EXPLAIN ANALYZE` disponible dans `duckdb`. Elle d√©compose le plan d'ex√©cution de `duckdb`, ce qui nous permettra de comprendre la strat√©gie d'optimisation. Elle permet aussi de conna√Ætre le volume de donn√©es import√©es lorsqu'on r√©cup√®re un fichier d'internet. En effet, `duckdb` est malin: plut√¥t que de t√©l√©charger un fichier entier pour n'en lire qu'une partie, la librairie est capable de n'importer que les blocs du fichier qui l'int√©resse.

Ceci n√©cessite l'utilisation de l'extension `httpfs` (un peu l'√©quivalent des `library` de `R` en `duckdb`). Elle s'installe et s'utilise de la mani√®re suivante

```{.r}
#| output: false
library(duckdb)

# url_bpe <- "https://www.insee.fr/fr/statistiques/fichier/8217525/BPE23.parquet"
url_bpe <- "https://minio.lab.sspcloud.fr/lgaliana/diffusion/BPE23.parquet"
con <- dbConnect(duckdb())

dbExecute(
  con,
  glue(
    "INSTALL httpfs;",
    "LOAD httpfs;"
  )
)
```

Demandons √† `DuckDB` d'ex√©cuter la requ√™te _"beaucoup de colonnes, pas beaucoup de lignes"_
et regardons le plan d'ex√©cution et les informations donn√©es par `DuckDB`:

<details>

<summary>
Voir le plan : _"beaucoup de colonnes, pas beaucoup de lignes"_
</summary>

```{.r}
glue(
    'EXPLAIN ANALYZE ',
    'SELECT * FROM read_parquet("{url_bpe}") LIMIT 5'
  )
```


```{.r}
plan <- dbGetQuery(
  con,
  glue(
    'EXPLAIN ANALYZE ',
    'SELECT * FROM read_parquet("{url_bpe}") LIMIT 5'
  )
)
```

```{.r}
print(plan)
```

</details>

<details>



<summary>
Voir le plan : _"peu de colonnes, beaucoup de lignes"_
</summary>

```{.r}
plan <- dbGetQuery(
  con,
  glue(
    'EXPLAIN ANALYZE ',
    'SELECT TYPEQU, LONGITUDE, LATITUDE FROM read_parquet("{url_bpe}") LIMIT 10000'
  )
)
```

```{.r}
print(plan)
```

</details>

La comparaison de ces plans d'ex√©cution montre l'int√©r√™t de faire un filtre sur les colonnes : les besoins computationnels sont drastiquement diminu√©s. Le filtre sur les lignes n'arrive que dans un second temps, une fois les colonnes s√©lectionn√©es.

Pourquoi seulement un rapport de 1 √† 4 entre le poids des deux fichiers ? C'est parce que nos requ√™tes comportent toute deux la variable `IPONDI` (les poids √† utiliser pour extrapoler l'√©chantillon √† la population) qui est √† haute pr√©cision l√† o√π beaucoup d'autres colonnes comportent un nombre r√©duit de modalit√©s et sont donc peu volumineuses.



# Le partitionnement

La *lazy evaluation* et les optimisations d'`Arrow` apportent des gain de performance consid√©rables. Mais on peut encore faire mieux ! Lorsqu'on sait qu'on va √™tre amen√© √† **filter r√©guli√®rement les donn√©es selon une variable d'int√©r√™t**, on a tout int√©r√™t √† **partitionner** le fichier `Parquet` selon cette variable.


::: {.exercise}
## Partie 3 : Le `Parquet` partitionn√© {.unnumbered}

* Parcourir la documentation de la fonction [arrow::write_dataset](https://arrow.apache.org/docs/r/reference/write_dataset.html) pour comprendre comment sp√©cifier la cl√© de partitionnement d'un fichier `Parquet`. Plusieurs m√©thodes sont possibles !
* Importer la table individus compl√®te du recensement `data/RPindividus.parquet` avec la fonction [arrow::open_dataset](https://arrow.apache.org/docs/r/reference/open_dataset.html) et l'exporter en une table `data/RPindividus_partitionne.parquet` partitionn√©e par la r√©gion (`REGION`) et le d√©partement (`DEPT`)
* Observer l'arborescence de fichiers de la table export√©e
* Modifier la fonction `req_open_dataset` de la partie pr√©c√©dente pour partir de la table compl√®te (non-partitionn√©e) `data/RPindividus.parquet` au lieu de l'√©chantillon
* Construire une fonction `req_open_dataset_partitionne` sur le mod√®le de `req_open_dataset`, qui importe cette fois les donn√©es partitionn√©es `data/RPindividus_partitionne.parquet`. Ne pas oublier de sp√©cifier le param√®tre `hive_style = TRUE`.
* Comparer les performances (temps d'ex√©cution et allocation m√©moire) des deux m√©thodes gr√¢ce √† la fonction [bench::mark](https://bench.r-lib.org/#benchmark)

:::

<iframe src="./bench/mark3.html" style="width: 100%; border-style: none; height: 0; overflow: hidden"
        onload="resizeIframe(this)"></iframe>


# `DuckDB` ou `Arrow` ? 

La r√©ponse d√©pend des pr√©f√©rences de chacun. Les deux √©cosyst√®mes sont tr√®s bien. `DuckDB` est pens√© pour sa simplicit√© d'usage et son universalit√©: en principe, qu'on fasse du {{< fa brands python >}}, {{< fa brands r-project >}}, {{< fa brands js-square >}} ou de la ligne de commande, on pourra utiliser le m√™me code SQL, seule la d√©finition de la connection changera. `Arrow` propose une syntaxe un peu moins famili√®re mais l'int√©gration au `tidyverse` rend ce _framework_ beaucoup plus facile d'usage que si on devait directement utiliser `Arrow`.

Il existe aussi des clients plus hauts niveau pour `duckdb`: c'est notamment le cas de la connection entre `duckdb` et `tidyverse` permise en {{< fa brands r-project >}}. Ceux-ci permettent des op√©rations de manipulation de donn√©es plus complexes: si le SQL de `DuckDB` est d√©j√† plus simple d'usage que celui de `PostGre`, on reste limit√© pour des op√©rations complexes de (d√©)structuration de donn√©es: le client `tidyverse` apporte les avantages de cet √©cosyst√®me en plus de ceux de `DuckDB`. 


# `(Geo)Parquet` et `DuckDB` pour les donn√©es spatiales


Nous proposons, pour illustrer l'un des atouts de `DuckDB`, √† savoir sa simplicit√© d'usage sur des probl√®mes complexes, de faire du traitement de donn√©es spatiales. Pour cela, nous allons faire une t√¢che lourde: restreindre des donn√©es spatiales √† partir de leur appartenance √† une zone g√©ographique donn√©e. Cette derni√®re sera d√©finie g√©om√©triquement comme un triangle dont deux des coins correspondent aux anciens b√¢timents de l'Insee √† Malakoff. 

L'import des contours dont nous aurons besoin en {{< fa brands r-project >}} se fait assez naturellement gr√¢ce √† [`sf`](https://r-spatial.github.io/sf/).

## R√©cup√©ration des donn√©es

```{.r}
#| output: false
dir.create("data")

download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/triangle.geojson", "data/triangle.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/malakoff.geojson", "data/malakoff.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/montrouge.geojson", "data/montrouge.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet", "data/dvf.parquet")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet", "data/carreaux.parquet")


triangle <- sf::st_read("data/triangle.geojson", quiet=TRUE)
malakoff <- sf::st_read("data/malakoff.geojson", quiet=TRUE)
montrouge <- sf::st_read("data/montrouge.geojson", quiet=TRUE)
```

On peut visualiser la ville de `Malakoff` et notre zone d'int√©r√™t (qu'on nommera, par abus de langage, le triangle d'or de Malakoff):

```{.r}
mapview(malakoff) + mapview(triangle, col.regions = "#ffff00")
```

Nous utiliserons aussi les contours de Montrouge pour cette partie:

```{.r}
mapview(montrouge)
```


En principe, `duckdb` fonctionne √† la mani√®re d'une base de donn√©es. Autrement dit, on d√©finit une base de donn√©es et effectue des requ√™tes (SQL ou verbes `tidyverse`) dessus. Pour cr√©er une base de donn√©es, il suffit de faire un `read_parquet` avec le chemin du fichier.

Comme il n'est pas possible de distinguer cette zone par requ√™tes attributaires, nous proposons de :

1. Via `DuckDB`, extraire les transactions de l'ensemble de la commune de Malakoff tout en conservant leur caract√®re spatial (chaque transaction correspond √† un point g√©ographique, avec ses coordonn√©es xy).
2. Utiliser localement le package `sf` pour distinguer spatialement les transactions effectu√©es √† l'int√©rieur ou √† l'ext√©rieur du Triangle d'Or (dont nous fournissons les contours).
3. Calculer la m√©diane des prix dans les deux sous-zones.

::: {.callout-tip collapse="true"}
## Format des g√©om√©tries
On extrait les transactions de Malakoff. Pour information, dans le fichier `dvf.parquet`, les coordonn√©es spatiales sont stock√©es dans un format binaire sp√©cifique (Well-Known Binary - WKB). Ce format est efficace pour le stockage et les calculs, mais n'est pas directement lisible ou interpr√©table par les humains.


En transformant ces g√©om√©tries en une repr√©sentation texte lisible (Well-Known Text - WKT) avec `ST_AsText`, on rend les donn√©es spatiales faciles √† afficher, interpr√©ter ou manipuler dans des contextes qui ne supportent pas directement les formats binaires g√©ospatiaux.
:::




## _Spatial join_ avec `DuckDB`

Pour cet exercice, nous allons utiliser les variables suivantes:

```{.r}
cog_malakoff <- "92046"
cog_montrouge <- "92049"
```

et le geoparquet pourra √™tre interpr√©t√© par `duckdb` selon ce mod√®le:

```{.r}
FROM read_parquet('data/dvf.parquet')
SELECT
  XXXX,
  ST_AsText(geometry) AS geom_text
WHERE XXXX
```

La base de donn√©es se cr√©e tout simplement de la mani√®re suivante :

```{.r}
#| output: false
#| echo: true

con <- dbConnect(duckdb::duckdb())
dbExecute(con, "INSTALL spatial;")
dbExecute(con, "LOAD spatial;")
```


::: {.exercise}
## Exercice 3 {.unnumbered}

1. En vous inspirant du _template_ ci-dessus, cr√©er un _dataframe_ `transactions_malakoff` qui recense les transactions dans cette charmante bourgade.

2. A ce niveau, les transactions extraites sont maintenant charg√©es en m√©moire et on les transforme dans un format qui facilite leur manipulation en R via le package `sf`.

```{.r}
transactions_malakoff <-
  sf::st_as_sf(transactions_malakoff, wkt = "geom_text", crs = 2154) |>
  rename(geometry=geom_text)
```

3. Nous allons cr√©er un masque pour reconna√Ætre les transactions qui sont situ√©es ou non dans le triangle d'or. Utiliser la structure suivante pour cr√©er ce masque :

```{.r}
bool_mask <- transactions_malakoff |>
  # ... |>
  sf::st_intersects(triangle, sparse = FALSE)
```

‚ö†Ô∏è il faut tenir compte des projections g√©ographiques avant de faire l'op√©ration d'intersection. Ce code est donc √† amender √† la marge pour pouvoir faire l'intersection.

Cela donne un vecteur de bool√©en, on peut donc identifier les transactions dans le triangle d'or ou en dehors √† partir de celui-ci.

:::

Ci-dessous le dataframe brut extrait via Duckdb (r√©ponse 1).

```{.r}
#| echo: true
query2 <- glue("
    FROM read_parquet('data/dvf.parquet')
    SELECT
        code_commune,
        valeur_fonciere,
        ST_AsText(geometry) AS geom_text
    WHERE code_commune = '{cog_malakoff}'
")

transactions_malakoff <- dbGetQuery(con, query2)

head(transactions_malakoff, 3)
```

Ci-dessous, le dataframe transform√© en objet `sf` et pr√™t pour les op√©rations spatiales (r√©ponse 2) :

```{.r}
#| echo: true
transactions_malakoff <-
  sf::st_as_sf(transactions_malakoff, wkt = "geom_text", crs = 2154) |>
  rename(geometry=geom_text)

head(transactions_malakoff, 3)
```

Une fois les donn√©es pr√™tes, on intersecte les points avec le triangle repr√©sentant le centre-ville de Malakoff (question 3)


```{.r}
#| echo: true
bool_mask <- transactions_malakoff |>
  sf::st_transform(4326) |>
  sf::st_intersects(triangle, sparse = FALSE)

head(bool_mask)
```

On peut ensuite facilement cr√©er nos deux espaces de Malakoff :

```{.r}
#| echo: true
in_triangle <- transactions_malakoff[bool_mask,]
out_triangle <- transactions_malakoff[!bool_mask,]
```

Une fois que chaque transaction est identifi√©e comme √©tant √† l'int√©rieur ou √† l'ext√©rieur du Triangle, le calcul de la m√©diane des prix est imm√©diat.

```{.r}
median_in <- median(in_triangle$valeur_fonciere)
median_out <- median(out_triangle$valeur_fonciere)

print(glue("M√©diane des prix dans le Triangle d'Or de Malakoff : ", median_in))
print(glue("M√©diane des prix dans le reste de Malakoff : ", median_out))
```

La m√©diane des prix est un peu plus √©lev√©e dans le Triangle qu'en dehors. On peut aller au-del√† et √©tudier la distribution des transactions. Bien que la taille d'√©chantillon soit r√©duite, on a ainsi une id√©e de la diversit√© des prix dans cette bucolique commune de Malakoff.


```{.r}
#| code-fold: true
#| code-summary: "Produire la figure sur la distribution du prix des biens"
library(ggplot2)
library(scales)

malakoff_identified <- transactions_malakoff %>%
  mutate(
    region = if_else(as.logical(bool_mask), "Triangle d'or", "Hors triangle d'or")
  )

ggplot(
  malakoff_identified,
  aes(y = valeur_fonciere, x = region, fill = region)
) +
  geom_violin() +
  scale_y_continuous(
    trans = "log10",
    labels = comma_format(),
    breaks = scales::trans_breaks("log10", function(x) 10^x)
  ) +
  geom_jitter(height = 0, width = 0.1) +
  labs(y = "Valeur de vente (‚Ç¨)") +
  theme_minimal()
```

Tout ceci ne nous dit rien de la diff√©rence entre les biens dans le triangle et en dehors de celui-ci. Nous n'avons fait aucun contr√¥le sur les caract√©ristiques des biens. Nous laissons les curieux explorer la mine d'or qu'est cette base.


# Acc√©der directement √† des donn√©es sur `S3`

{{< include "./python/_s3.qmd" >}}

# DuckDB WASM

`DuckDB` permet aussi de faire des traitements analytiques directement dans le navigateur gr√¢ce √† son impl√©mentation WASM. 
C'est particuli√®rement pratique pour cr√©er des applications r√©actives comme celle-ci :

```{ojs}
//| echo: false
html`
  <div style="display: flex; flex-direction: column; gap: 1rem;">

    <!-- Search bar at the top -->
    <div>${viewof search}</div>

    <!-- Two-column block -->
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; backgroundColor: '#293845';">
      <div>${produce_histo(dvf)}</div>
      <div>${viewof table_dvf}</div>
    </div>


  </div>
`
```

```{ojs}
//| echo: false
{
  const container = html`<link
    rel="stylesheet"
    href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css"
  />
  <script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js"></script>
  <div id="map" style="height: 500px;"></div>`

  yield container

  const map = L.map('map');

  L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {
    attribution: '¬© OpenStreetMap contributors'
  }).addTo(map);

  const geoLayer = L.geoJSON(dvf_geojson, {
    onEachFeature: (feature, layer) => {
      const props = feature.properties;
      const popupContent = `
        <strong>Date:</strong> ${props.date}<br>
        <strong>Valeur fonci√®re:</strong> ${props.valeur_fonciere} ‚Ç¨<br>
        <strong>Commune:</strong> ${props.code_commune}
      `;
      layer.bindPopup(popupContent);
    }
  }).addTo(map);

  // üëá Automatically fit view to all points
  map.fitBounds(geoLayer.getBounds());
}
```

{{< include "_interactive_parquet.qmd" >}}


```{ojs}
//| echo: false
viewof table_dvf = Inputs.table(dvf, {columns: ["date", "valeur_fonciere"], rows: 15})

produce_histo = function(dvf){
  const histo = Plot.plot({
  style: {backgroundColor: "transparent"},
  marks: [
    Plot.rectY(dvf, Plot.binX({y: "count"}, {x: "valeur_fonciere", fill: "#ff562c"})),
    Plot.ruleY([0])
  ]
})
  return histo
}
```


```{ojs}
//| echo: false
dvf_geojson = {
 
  const geojson = {
    type: "FeatureCollection",
    features: dvf.map(row => ({
      type: "Feature",
      geometry: {
        type: "Point",
        coordinates: [row.longitude, row.latitude],
      },
      properties: {
        date: row.date,
        valeur_fonciere: row.valeur_fonciere,
        code_commune: row.code_commune,
        valeur_fonciere_bar: row.valeur_fonciere_bar,
      },
    })),
  };

  return geojson
}
```

Que ce soit avec `Quarto` (comme ici) ou par le biais d'`Observable` (comme [l√†](https://observablehq.com/@linogaliana/atelier-parquet-ssphub)), il est donc possible de construire des applications interactives reposant sur des sites statiques, sans avoir besoin d'un serveur {{< fa brands r-project >}} ou {{< fa brands python >}}.


# R√©f√©rences suppl√©mentaires {-}

Du contenu p√©dagogique suppl√©mentaire sur le sujet, produit par l'Insee:

- [La formation aux bonnes pratiques](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/) de l'Insee
- Un atelier de l'EHESS sur `Parquet` avec de nombreux exemples [ici](https://linogaliana.github.io/parquet-recensement-tutomate/)
- Le [cours de mise en production](https://ensae-reproductibilite.github.io/website/) de l'ENSAE

D'autres r√©f√©rences utiles, √† consommer sans mod√©ration, sur le sujet:

- Les posts d'Eric Mauvi√®re sur [icem7.fr/](https://www.icem7.fr/)
- [Webinaire du CASD](https://www.casd.eu/webinaire-casd-data-tech/) sur `Parquet` et `DuckDB`


Une vid√©o sur la philosophie derri√®re la naissance de `DuckDB` et sa relation avec `MotherDuck`: 

{{< video https://www.youtube.com/watch?v=JH4Th2dFSUk >}}
