[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Atelier pour découvrir la récupération de données via avec le format Parquet",
    "section": "",
    "text": "Tout au long de ce tutoriel guidé, nous allons voir comment utiliser le format Parquet de manière la plus efficiente.\nAfin de comparer les différents formats et méthodes d’utilisation, nous allons généralement comparer le temps d’exécution et l’usage mémoire d’une requête standard.\n\n\nAu cours de cet atelier, nous aurons besoin des packages suivants:\n#| output: false\nlibrary(duckdb)\nlibrary(glue)\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(mapview)\nCe tutoriel s’appuie sur des données ouvertes diffusées au format Parquet. Pour les récupérer, vous pouvez exécuter le script suivant:\n\n\nRécupérer les données\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(fs)\n\n# Chemin du fichier d'entrée\nfilename_table_individu &lt;- \"data/RPindividus.parquet\"\n\n# Lire le fichier Parquet\ndf &lt;- read_parquet(filename_table_individu)\n\n# Filtrer les données pour REGION == \"24\"\ndf_filtered &lt;- df %&gt;% filter(REGION == \"24\")\n\n# Sauvegarder en CSV\nwrite_csv(df_filtered, \"data/RPindividus_24.csv\")\n\n# Sauvegarder en Parquet\nwrite_parquet(df_filtered, \"data/RPindividus_24.parquet\")\n\n# Créer le dossier si nécessaire\ndir_create(\"data/RPindividus\")\n\n# Sauvegarder en Parquet partitionné par REGION et DEPT\nwrite_dataset(\n  df,\n  path = \"data/RPindividus\",\n  format = \"parquet\",\n  partitioning = c(\"REGION\", \"DEPT\")\n)"
  },
  {
    "objectID": "index.html#etapes-préliminaires",
    "href": "index.html#etapes-préliminaires",
    "title": "Atelier pour découvrir la récupération de données via avec le format Parquet",
    "section": "",
    "text": "Au cours de cet atelier, nous aurons besoin des packages suivants:\n#| output: false\nlibrary(duckdb)\nlibrary(glue)\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(mapview)\nCe tutoriel s’appuie sur des données ouvertes diffusées au format Parquet. Pour les récupérer, vous pouvez exécuter le script suivant:\n\n\nRécupérer les données\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(fs)\n\n# Chemin du fichier d'entrée\nfilename_table_individu &lt;- \"data/RPindividus.parquet\"\n\n# Lire le fichier Parquet\ndf &lt;- read_parquet(filename_table_individu)\n\n# Filtrer les données pour REGION == \"24\"\ndf_filtered &lt;- df %&gt;% filter(REGION == \"24\")\n\n# Sauvegarder en CSV\nwrite_csv(df_filtered, \"data/RPindividus_24.csv\")\n\n# Sauvegarder en Parquet\nwrite_parquet(df_filtered, \"data/RPindividus_24.parquet\")\n\n# Créer le dossier si nécessaire\ndir_create(\"data/RPindividus\")\n\n# Sauvegarder en Parquet partitionné par REGION et DEPT\nwrite_dataset(\n  df,\n  path = \"data/RPindividus\",\n  format = \"parquet\",\n  partitioning = c(\"REGION\", \"DEPT\")\n)"
  },
  {
    "objectID": "index.html#la-lazy-evaluation-en-pratique",
    "href": "index.html#la-lazy-evaluation-en-pratique",
    "title": "Atelier pour découvrir la récupération de données via avec le format Parquet",
    "section": "3.1 La lazy evaluation en pratique",
    "text": "3.1 La lazy evaluation en pratique\n\n\n\n\n\n\nExercice 2 : Exploiter la lazy evaluation et les optimisations d’Arrow\n\n\n\n\nUtiliser la fonction arrow::open_dataset pour ouvrir le fichier data/RPindividus_24.parquet. Regarder la classe de l’objet obtenu.\nAfficher les 5 premières lignes de la table avec la fonction head(). Observer l’objet obtenu (sortie en console, classe).\nFaire la même chose avec duckdb (par le biais de l’API tidyverse et en SQL direct)\nPrenez ce code:\n\narrow::open_dataset(path_parquet_subset) |&gt;\n    filter(DEPT == \"36\") |&gt;\n    group_by(AGED, DEPT) |&gt;\n    summarise(n_indiv = sum(IPONDI))\net exécutez le. Que se passe-t-il à votre avis ?\n\nAjouter une étape collect() à la fin de cette chaîne. Comprenez-vous la différence ?\nConstruire une fonction req_open_dataset sur le modèle de celles de la partie précédente, qui importe cette fois les données avec la fonction arrow::open_dataset\nComparer les performances (temps d’exécution et allocation mémoire) de la méthode read_parquet et de la méthode open_dataset grâce à la fonction bench::mark"
  },
  {
    "objectID": "index.html#comprendre-loptimisation-permise-par-parquet-et-duckdb",
    "href": "index.html#comprendre-loptimisation-permise-par-parquet-et-duckdb",
    "title": "Atelier pour découvrir la récupération de données via avec le format Parquet",
    "section": "3.2 Comprendre l’optimisation permise par Parquet et DuckDB",
    "text": "3.2 Comprendre l’optimisation permise par Parquet et DuckDB\nPour réduire la volumétrie des données importées, il est possible de mettre en oeuvre deux stratégies:\n\nN’importer qu’un nombre limité de colonnes\nN’importer qu’un nombre limité de lignes\n\nComme cela a été évoqué dans les slides, le format Parquet est particulièrement optimisé pour le premier besoin. C’est donc généralement la première optimisation mise en oeuvre. Pour s’en convaincre on peut regarder la taille des données importées dans deux cas:\n\nOn utilise beaucoup de lignes mais peu de colonnes\nOn utilise beaucoup de colonnes mais peu de lignes\n\nPour cela, nous utilisons la fonction SQL EXPLAIN ANALYZE disponible dans duckdb. Elle décompose le plan d’exécution de duckdb, ce qui nous permettra de comprendre la stratégie d’optimisation. Elle permet aussi de connaître le volume de données importées lorsqu’on récupère un fichier d’internet. En effet, duckdb est malin: plutôt que de télécharger un fichier entier pour n’en lire qu’une partie, la librairie est capable de n’importer que les blocs du fichier qui l’intéresse.\nCeci nécessite l’utilisation de l’extension httpfs (un peu l’équivalent des library de R en duckdb). Elle s’installe et s’utilise de la manière suivante\n#| output: false\nlibrary(duckdb)\n\n# url_bpe &lt;- \"https://www.insee.fr/fr/statistiques/fichier/8217525/BPE23.parquet\"\nurl_bpe &lt;- \"https://minio.lab.sspcloud.fr/lgaliana/diffusion/BPE23.parquet\"\ncon &lt;- dbConnect(duckdb())\n\ndbExecute(\n  con,\n  glue(\n    \"INSTALL httpfs;\",\n    \"LOAD httpfs;\"\n  )\n)\nDemandons à DuckDB d’exécuter la requête “beaucoup de colonnes, pas beaucoup de lignes” et regardons le plan d’exécution et les informations données par DuckDB:\n\n\nVoir le plan : “beaucoup de colonnes, pas beaucoup de lignes”\n\nglue(\n    'EXPLAIN ANALYZE ',\n    'SELECT * FROM read_parquet(\"{url_bpe}\") LIMIT 5'\n  )\nplan &lt;- dbGetQuery(\n  con,\n  glue(\n    'EXPLAIN ANALYZE ',\n    'SELECT * FROM read_parquet(\"{url_bpe}\") LIMIT 5'\n  )\n)\nprint(plan)\n\n\n\nVoir le plan : “peu de colonnes, beaucoup de lignes”\n\nplan &lt;- dbGetQuery(\n  con,\n  glue(\n    'EXPLAIN ANALYZE ',\n    'SELECT TYPEQU, LONGITUDE, LATITUDE FROM read_parquet(\"{url_bpe}\") LIMIT 10000'\n  )\n)\nprint(plan)\n\nLa comparaison de ces plans d’exécution montre l’intérêt de faire un filtre sur les colonnes : les besoins computationnels sont drastiquement diminués. Le filtre sur les lignes n’arrive que dans un second temps, une fois les colonnes sélectionnées.\nPourquoi seulement un rapport de 1 à 4 entre le poids des deux fichiers ? C’est parce que nos requêtes comportent toute deux la variable IPONDI (les poids à utiliser pour extrapoler l’échantillon à la population) qui est à haute précision là où beaucoup d’autres colonnes comportent un nombre réduit de modalités et sont donc peu volumineuses."
  },
  {
    "objectID": "index.html#récupération-des-données",
    "href": "index.html#récupération-des-données",
    "title": "Atelier pour découvrir la récupération de données via avec le format Parquet",
    "section": "6.1 Récupération des données",
    "text": "6.1 Récupération des données\n#| output: false\ndir.create(\"data\")\n\ndownload.file(\"https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/triangle.geojson\", \"data/triangle.geojson\")\ndownload.file(\"https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/malakoff.geojson\", \"data/malakoff.geojson\")\ndownload.file(\"https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/montrouge.geojson\", \"data/montrouge.geojson\")\ndownload.file(\"https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet\", \"data/dvf.parquet\")\ndownload.file(\"https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet\", \"data/carreaux.parquet\")\n\n\ntriangle &lt;- sf::st_read(\"data/triangle.geojson\", quiet=TRUE)\nmalakoff &lt;- sf::st_read(\"data/malakoff.geojson\", quiet=TRUE)\nmontrouge &lt;- sf::st_read(\"data/montrouge.geojson\", quiet=TRUE)\nOn peut visualiser la ville de Malakoff et notre zone d’intérêt (qu’on nommera, par abus de langage, le triangle d’or de Malakoff):\nmapview(malakoff) + mapview(triangle, col.regions = \"#ffff00\")\nNous utiliserons aussi les contours de Montrouge pour cette partie:\nmapview(montrouge)\nEn principe, duckdb fonctionne à la manière d’une base de données. Autrement dit, on définit une base de données et effectue des requêtes (SQL ou verbes tidyverse) dessus. Pour créer une base de données, il suffit de faire un read_parquet avec le chemin du fichier.\nComme il n’est pas possible de distinguer cette zone par requêtes attributaires, nous proposons de :\n\nVia DuckDB, extraire les transactions de l’ensemble de la commune de Malakoff tout en conservant leur caractère spatial (chaque transaction correspond à un point géographique, avec ses coordonnées xy).\nUtiliser localement le package sf pour distinguer spatialement les transactions effectuées à l’intérieur ou à l’extérieur du Triangle d’Or (dont nous fournissons les contours).\nCalculer la médiane des prix dans les deux sous-zones.\n\n\n\n\n\n\n\nFormat des géométries\n\n\n\n\n\nOn extrait les transactions de Malakoff. Pour information, dans le fichier dvf.parquet, les coordonnées spatiales sont stockées dans un format binaire spécifique (Well-Known Binary - WKB). Ce format est efficace pour le stockage et les calculs, mais n’est pas directement lisible ou interprétable par les humains.\nEn transformant ces géométries en une représentation texte lisible (Well-Known Text - WKT) avec ST_AsText, on rend les données spatiales faciles à afficher, interpréter ou manipuler dans des contextes qui ne supportent pas directement les formats binaires géospatiaux."
  },
  {
    "objectID": "index.html#spatial-join-avec-duckdb",
    "href": "index.html#spatial-join-avec-duckdb",
    "title": "Atelier pour découvrir la récupération de données via avec le format Parquet",
    "section": "6.2 Spatial join avec DuckDB",
    "text": "6.2 Spatial join avec DuckDB\nPour cet exercice, nous allons utiliser les variables suivantes:\ncog_malakoff &lt;- \"92046\"\ncog_montrouge &lt;- \"92049\"\net le geoparquet pourra être interprété par duckdb selon ce modèle:\nFROM read_parquet('data/dvf.parquet')\nSELECT\n  XXXX,\n  ST_AsText(geometry) AS geom_text\nWHERE XXXX\nLa base de données se crée tout simplement de la manière suivante :\n#| output: false\n#| echo: true\n\ncon &lt;- dbConnect(duckdb::duckdb())\ndbExecute(con, \"INSTALL spatial;\")\ndbExecute(con, \"LOAD spatial;\")\n\n\n\n\n\n\nExercice 3\n\n\n\n\nEn vous inspirant du template ci-dessus, créer un dataframe transactions_malakoff qui recense les transactions dans cette charmante bourgade.\nA ce niveau, les transactions extraites sont maintenant chargées en mémoire et on les transforme dans un format qui facilite leur manipulation en R via le package sf.\n\ntransactions_malakoff &lt;-\n  sf::st_as_sf(transactions_malakoff, wkt = \"geom_text\", crs = 2154) |&gt;\n  rename(geometry=geom_text)\n\nNous allons créer un masque pour reconnaître les transactions qui sont situées ou non dans le triangle d’or. Utiliser la structure suivante pour créer ce masque :\n\nbool_mask &lt;- transactions_malakoff |&gt;\n  # ... |&gt;\n  sf::st_intersects(triangle, sparse = FALSE)\n⚠️ il faut tenir compte des projections géographiques avant de faire l’opération d’intersection. Ce code est donc à amender à la marge pour pouvoir faire l’intersection.\nCela donne un vecteur de booléen, on peut donc identifier les transactions dans le triangle d’or ou en dehors à partir de celui-ci.\n\n\nCi-dessous le dataframe brut extrait via Duckdb (réponse 1).\n#| echo: true\nquery2 &lt;- glue(\"\n    FROM read_parquet('data/dvf.parquet')\n    SELECT\n        code_commune,\n        valeur_fonciere,\n        ST_AsText(geometry) AS geom_text\n    WHERE code_commune = '{cog_malakoff}'\n\")\n\ntransactions_malakoff &lt;- dbGetQuery(con, query2)\n\nhead(transactions_malakoff, 3)\nCi-dessous, le dataframe transformé en objet sf et prêt pour les opérations spatiales (réponse 2) :\n#| echo: true\ntransactions_malakoff &lt;-\n  sf::st_as_sf(transactions_malakoff, wkt = \"geom_text\", crs = 2154) |&gt;\n  rename(geometry=geom_text)\n\nhead(transactions_malakoff, 3)\nUne fois les données prêtes, on intersecte les points avec le triangle représentant le centre-ville de Malakoff (question 3)\n#| echo: true\nbool_mask &lt;- transactions_malakoff |&gt;\n  sf::st_transform(4326) |&gt;\n  sf::st_intersects(triangle, sparse = FALSE)\n\nhead(bool_mask)\nOn peut ensuite facilement créer nos deux espaces de Malakoff :\n#| echo: true\nin_triangle &lt;- transactions_malakoff[bool_mask,]\nout_triangle &lt;- transactions_malakoff[!bool_mask,]\nUne fois que chaque transaction est identifiée comme étant à l’intérieur ou à l’extérieur du Triangle, le calcul de la médiane des prix est immédiat.\nmedian_in &lt;- median(in_triangle$valeur_fonciere)\nmedian_out &lt;- median(out_triangle$valeur_fonciere)\n\nprint(glue(\"Médiane des prix dans le Triangle d'Or de Malakoff : \", median_in))\nprint(glue(\"Médiane des prix dans le reste de Malakoff : \", median_out))\nLa médiane des prix est un peu plus élevée dans le Triangle qu’en dehors. On peut aller au-delà et étudier la distribution des transactions. Bien que la taille d’échantillon soit réduite, on a ainsi une idée de la diversité des prix dans cette bucolique commune de Malakoff.\n#| code-fold: true\n#| code-summary: \"Produire la figure sur la distribution du prix des biens\"\nlibrary(ggplot2)\nlibrary(scales)\n\nmalakoff_identified &lt;- transactions_malakoff %&gt;%\n  mutate(\n    region = if_else(as.logical(bool_mask), \"Triangle d'or\", \"Hors triangle d'or\")\n  )\n\nggplot(\n  malakoff_identified,\n  aes(y = valeur_fonciere, x = region, fill = region)\n) +\n  geom_violin() +\n  scale_y_continuous(\n    trans = \"log10\",\n    labels = comma_format(),\n    breaks = scales::trans_breaks(\"log10\", function(x) 10^x)\n  ) +\n  geom_jitter(height = 0, width = 0.1) +\n  labs(y = \"Valeur de vente (€)\") +\n  theme_minimal()\nTout ceci ne nous dit rien de la différence entre les biens dans le triangle et en dehors de celui-ci. Nous n’avons fait aucun contrôle sur les caractéristiques des biens. Nous laissons les curieux explorer la mine d’or qu’est cette base."
  },
  {
    "objectID": "index.html#données-partitionnées",
    "href": "index.html#données-partitionnées",
    "title": "Atelier pour découvrir la récupération de données via avec le format Parquet",
    "section": "7.1 Données partitionnées",
    "text": "7.1 Données partitionnées\nOn peut faire la même chose avec des données partitionnées:\n\ncon.sql(\"SELECT * FROM read_parquet('s3://projet-formation/bonnes-pratiques/data/RPindividus/**/*.parquet', hive_partitioning = true) WHERE DEPT IN (11, 31, 34)\")"
  }
]