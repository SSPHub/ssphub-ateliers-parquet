---
title: "Atelier pour découvrir la récupération de données via avec le format Parquet"
author: Lino Galiana
date: 2025-04-09
description: |
  XXXXX
number-sections: true
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/serveurpython.jpg
---

Tout au long de cette application, nous allons voir comment utiliser le format `Parquet` de manière la plus efficiente.

Afin de comparer les différents formats et méthodes d'utilisation, nous allons **comparer le temps d'exécution et l'usage mémoire d'une requête standard**.


## Etapes préliminaires

```{python}
from utils_parquet import download_dataset_mc, measure_performance
```

```{python}
download_dataset_mc(engine="mc")
```


## Exo 1. CSV -> Parquet + intérêt lazy evaluation

Commençons par comparer les formats `CSV` et `Parquet`.


:::{.exercise}
# Exercice 1 : Du `CSV` au `Parquet`

* La requête suivante permet de calculer les données pour construire une pyramide des âges sur un département donné, à partir du fichier `CSV` du recensement. Encapsuler la requête dans une fonction `req_csv` (sans argument).

```{.r}
res <- readr::read_csv("data/RPindividus_24.csv") |>
    filter(DEPT == "36") |>
    group_by(AGED, DEPT) |>
    summarise(n_indiv = sum(IPONDI))
```

* Sur le même modèle, construire une fonction `req_read_parquet` basée cette fois sur le fichier `data/RPindividus_24.parquet` chargé avec la fonction [read_parquet](https://arrow.apache.org/docs/r/reference/read_parquet.html) d'`Arrow`

* Comparer les performances (temps d'exécution et allocation mémoire) de ces deux méthodes grâce à la fonction [bench::mark](https://bench.r-lib.org/#benchmark), à laquelle on passera les paramètres `iterations = 1` (comparaison à partir d'une seule itération) et `check = FALSE` (autorise les outputs des deux fonctions à être différents).

* Arranger les données pour avoir un tableau de résultats
:::


## Exo 2: lazy evaluation

La partie précédente a montré un **gain de temps considérable** du passage de `CSV` à `Parquet`. Néanmoins, l'**utilisation mémoire était encore très élevée** alors qu'on utilise de fait qu'une infime partie du fichier. Dans cette partie, on va voir comment utiliser la ***lazy evaluation*** et les **optimisations du plan d'exécution** effectuées par `Arrow` pour exploiter pleinement la puissance du format `Parquet`.

::: {.exercise}
## Exercice 2 : Exploiter la *lazy evaluation* et les optimisations d'`Arrow`

* Utiliser la fonction [arrow::open_dataset](https://arrow.apache.org/docs/r/reference/open_dataset.html) pour ouvrir le fichier `data/RPindividus_24.parquet`. Regarder la classe de l'objet obtenu.
* Afficher les 5 premières lignes de la table avec la fonction `head()`. Observer l'objet obtenu (sortie en console, classe).
* Faire la même chose avec `duckdb` (par le biais de l'API _tidyverse_ et en SQL direct)

* Prenez ce code:

```{.r}
arrow::open_dataset(path_parquet_subset) |>
    filter(DEPT == "36") |>
    group_by(AGED, DEPT) |>
    summarise(n_indiv = sum(IPONDI))
```

et exécutez le. Que se passe-t-il à votre avis ?

* Ajouter une étape `collect()` à la fin de cette chaîne. Comprenez-vous la différence ?

* Construire une fonction `req_open_dataset` sur le modèle de celles de la partie précédente, qui importe cette fois les données avec la fonction [arrow::open_dataset](https://arrow.apache.org/docs/r/reference/open_dataset.html)
* Comparer les performances (temps d'exécution et allocation mémoire) de la méthode `read_parquet` et de la méthode `open_dataset` grâce à la fonction [bench::mark](https://bench.r-lib.org/#benchmark)


:::

::: {#tip-optimisation-duckdb .callout-tip collapse="true"}
## Comprendre l'optimisation permise par `Parquet` et `DuckDB`

Pour réduire la volumétrie des données importées, il est possible de mettre en oeuvre deux stratégies:

- N'importer qu'un nombre limité de colonnes
- N'importer qu'un nombre limité de lignes

Comme cela a été évoqué dans les _slides_, le format `Parquet` est particulièrement optimisé pour le premier besoin. C'est donc généralement la première optimisation mise en oeuvre. Pour s'en convaincre on peut regarder la taille des données importées dans deux cas:

- On utilise beaucoup de lignes mais peu de colonnes
- On utilise beaucoup de colonnes mais peu de lignes

Pour cela, nous utilisons la fonction SQL `EXPLAIN ANALYZE` disponible dans `duckdb`. Elle décompose le plan d'exécution de `duckdb`, ce qui nous permettra de comprendre la stratégie d'optimisation. Elle permet aussi de connaître le volume de données importées lorsqu'on récupère un fichier d'internet. En effet, `duckdb` est malin: plutôt que de télécharger un fichier entier pour n'en lire qu'une partie, la librairie est capable de n'importer que les blocs du fichier qui l'intéresse.

Ceci nécessite l'utilisation de l'extension `httpfs` (un peu l'équivalent des `library` de `R` en `duckdb`). Elle s'installe et s'utilise de la manière suivante

```{.r}
#| output: false
dbExecute(
  con,
  glue(
    "INSTALL httpfs;",
    "LOAD httpfs;"
  )
)
```

Demandons à `DuckDB` d'exécuter la requête _"beaucoup de colonnes, pas beaucoup de lignes"_
et regardons le plan d'exécution et les informations données par `DuckDB`:

<details>

<summary>
Voir le plan : _"beaucoup de colonnes, pas beaucoup de lignes"_
</summary>

```{r}
glue(
    'EXPLAIN ANALYZE ',
    'SELECT * FROM read_parquet("{url_bpe}") LIMIT 5'
  )
```


```{r}
plan <- dbGetQuery(
  con,
  glue(
    'EXPLAIN ANALYZE ',
    'SELECT * FROM read_parquet("{url_bpe}") LIMIT 5'
  )
)
```

```{r}
print(plan)
```

</details>

<details>



<summary>
Voir le plan : _"peu de colonnes, beaucoup de lignes"_
</summary>

```{r}
plan <- dbGetQuery(
  con,
  glue(
    'EXPLAIN ANALYZE ',
    'SELECT TYPEQU, LONGITUDE, LATITUDE FROM read_parquet("{url_bpe}") LIMIT 10000'
  )
)
```

```{r}
print(plan)
```

</details>

La comparaison de ces plans d'exécution montre l'intérêt de faire un filtre sur les colonnes : les besoins computationnels sont drastiquement diminués. Le filtre sur les lignes n'arrive que dans un second temps, une fois les colonnes sélectionnées.

Pourquoi seulement un rapport de 1 à 4 entre le poids des deux fichiers ? C'est parce que nos requêtes comportent toute deux la variable `IPONDI` (les poids à utiliser pour extrapoler l'échantillon à la population) qui est à haute précision là où beaucoup d'autres colonnes comportent un nombre réduit de modalités et sont donc peu volumineuses.

:::


::: {.exercise}
# Partie 3 : Le `Parquet` partitionné

La *lazy evaluation* et les optimisations d'`Arrow` apportent des gain de performance considérables. Mais on peut encore faire mieux ! Lorsqu'on sait qu'on va être amené à **filter régulièrement les données selon une variable d'intérêt**, on a tout intérêt à **partitionner** le fichier `Parquet` selon cette variable.

* Parcourir la documentation de la fonction [arrow::write_dataset](https://arrow.apache.org/docs/r/reference/write_dataset.html) pour comprendre comment spécifier la clé de partitionnement d'un fichier `Parquet`. Plusieurs méthodes sont possibles !
* Importer la table individus complète du recensement `data/RPindividus.parquet` avec la fonction [arrow::open_dataset](https://arrow.apache.org/docs/r/reference/open_dataset.html) et l'exporter en une table `data/RPindividus_partitionne.parquet` partitionnée par la région (`REGION`) et le département (`DEPT`)
* Observer l'arborescence de fichiers de la table exportée
* Modifier la fonction `req_open_dataset` de la partie précédente pour partir de la table complète (non-partitionnée) `data/RPindividus.parquet` au lieu de l'échantillon
* Construire une fonction `req_open_dataset_partitionne` sur le modèle de `req_open_dataset`, qui importe cette fois les données partitionnées `data/RPindividus_partitionne.parquet`. Ne pas oublier de spécifier le paramètre `hive_style = TRUE`.
* Comparer les performances (temps d'exécution et allocation mémoire) des deux méthodes grâce à la fonction [bench::mark](https://bench.r-lib.org/#benchmark)

:::


## Exercice 4

L'import des contours dont nous aurons besoin en {{< fa brands r-project >}} se fait assez naturellement grâce à [`sf`](https://r-spatial.github.io/sf/).

```{r}
library(duckdb)
library(glue)
library(dplyr)
library(dbplyr)
library(mapview)
```

```{r}
triangle <- sf::st_read("data/triangle.geojson", quiet=TRUE)
malakoff <- sf::st_read("data/malakoff.geojson", quiet=TRUE)
montrouge <- sf::st_read("data/montrouge.geojson", quiet=TRUE)
```

En premier lieu, on peut visualiser la ville de `Malakoff` :

```{r}
mapview(malakoff) + mapview(triangle, col.regions = "#ffff00")
```

Et ensuite les contours de Montrouge :

```{r}
mapview(montrouge)
```


`DuckDB` est un moteur de base de données analytique en mémoire, optimisé pour les requêtes SQL sur des données volumineuses, particulièrement adapté aux fichiers plats comme `Parquet` ou CSV, et intégrable dans des langages comme Python, R ou SQL.

En principe, `duckdb` fonctionne à la manière d'une base de données. Autrement dit, on définit une base de données et effectue des requêtes (SQL ou verbes `tidyverse`) dessus. Pour créer une base de données, il suffit de faire un `read_parquet` avec le chemin du fichier.

La base de données se crée tout simplement de la manière suivante :

```{r}
#| output: false
#| echo: true

con <- dbConnect(duckdb::duckdb())
dbExecute(con, "INSTALL spatial;")
dbExecute(con, "LOAD spatial;")
```

Comme il n'est pas possible de distinguer cette zone par requêtes attributaires, nous proposons de :

1. Via `DuckDB`, extraire les transactions de l'ensemble de la commune de Malakoff tout en conservant leur caractère spatial (chaque transaction correspond à un point géographique, avec ses coordonnées xy).
2. Utiliser localement le package `sf` pour distinguer spatialement les transactions effectuées à l'intérieur ou à l'extérieur du Triangle d'Or (dont nous fournissons les contours).
3. Calculer la médiane des prix dans les deux sous-zones.

::: {.callout-tip collapse="true"}
## Format des géométries
On extrait les transactions de Malakoff. Pour information, dans le fichier `dvf.parquet`, les coordonnées spatiales sont stockées dans un format binaire spécifique (Well-Known Binary - WKB). Ce format est efficace pour le stockage et les calculs, mais n'est pas directement lisible ou interprétable par les humains.


En transformant ces géométries en une représentation texte lisible (Well-Known Text - WKT) avec `ST_AsText`, on rend les données spatiales faciles à afficher, interpréter ou manipuler dans des contextes qui ne supportent pas directement les formats binaires géospatiaux.
:::


```{r}
cog_malakoff <- "92046"
cog_montrouge <- "92049"
```

::: {.exercice}
## Exercice 3

1. En vous inspirant du _template_ ci-dessus, créer un _dataframe_ `transactions_malakoff` qui recense les transactions dans cette charmante bourgade.

2. A ce niveau, les transactions extraites sont maintenant chargées en mémoire et on les transforme dans un format qui facilite leur manipulation en R via le package `sf`.

```{.r}
transactions_malakoff <-
  sf::st_as_sf(transactions_malakoff, wkt = "geom_text", crs = 2154) |>
  rename(geometry=geom_text)
```

3. Nous allons créer un masque pour reconnaître les transactions qui sont situées ou non dans le triangle d'or. Utiliser la structure suivante pour créer ce masque :

```{.r}
bool_mask <- transactions_malakoff |>
  # ... |>
  sf::st_intersects(triangle, sparse = FALSE)
```

⚠️ il faut tenir compte des projections géographiques avant de faire l'opération d'intersection. Ce code est donc à amender à la marge pour pouvoir faire l'intersection.

Cela donne un vecteur de booléen, on peut donc identifier les transactions dans le triangle d'or ou en dehors à partir de celui-ci.

:::

Ci-dessous le dataframe brut extrait via Duckdb (réponse 1).

```{r}
#| echo: true
query2 <- glue("
    FROM read_parquet('data/dvf.parquet')
    SELECT
        code_commune,
        valeur_fonciere,
        ST_AsText(geometry) AS geom_text
    WHERE code_commune = '{cog_malakoff}'
")

transactions_malakoff <- dbGetQuery(con, query2)

transactions_malakoff
```

Ci-dessous, le dataframe transformé en objet `sf` et prêt pour les opérations spatiales (réponse 2) :

```{r}
#| echo: true
transactions_malakoff <-
  sf::st_as_sf(transactions_malakoff, wkt = "geom_text", crs = 2154) |>
  rename(geometry=geom_text)

transactions_malakoff
```

Une fois les données prêtes, on intersecte les points avec le triangle représentant le centre-ville de Malakoff (question 3)


```{r}
#| echo: true
bool_mask <- transactions_malakoff |>
  sf::st_transform(4326) |>
  sf::st_intersects(triangle, sparse = FALSE)

head(bool_mask)
```

On peut ensuite facilement créer nos deux espaces de Malakoff :

```{r}
#| echo: true
in_triangle <- transactions_malakoff[bool_mask,]
out_triangle <- transactions_malakoff[!bool_mask,]
```

Une fois que chaque transaction est identifiée comme étant à l'intérieur ou à l'extérieur du Triangle, le calcul de la médiane des prix est immédiat.

```{r}
median_in <- median(in_triangle$valeur_fonciere)
median_out <- median(out_triangle$valeur_fonciere)

print(glue("Médiane des prix dans le Triangle d'Or de Malakoff : ", median_in))
print(glue("Médiane des prix dans le reste de Malakoff : ", median_out))
```

La médiane des prix est un peu plus élevée dans le Triangle qu'en dehors. On peut aller au-delà et étudier la distribution des transactions. Bien que la taille d'échantillon soit réduite, on a ainsi une idée de la diversité des prix dans cette bucolique commune de Malakoff.


```{r}
#| code-fold: true
#| code-summary: "Produire la figure sur la distribution du prix des biens"
library(ggplot2)
library(scales)

malakoff_identified <- transactions_malakoff %>%
  mutate(
    region = if_else(as.logical(bool_mask), "Triangle d'or", "Hors triangle d'or")
  )

ggplot(
  malakoff_identified,
  aes(y = valeur_fonciere, x = region, fill = region)
) +
  geom_violin() +
  scale_y_continuous(
    trans = "log10",
    labels = comma_format(),
    breaks = scales::trans_breaks("log10", function(x) 10^x)
  ) +
  geom_jitter(height = 0, width = 0.1) +
  labs(y = "Valeur de vente (€)") +
  theme_minimal()
```

Tout ceci ne nous dit rien de la différence entre les biens dans le triangle et en dehors de celui-ci. Nous n'avons fait aucun contrôle sur les caractéristiques des biens. Nous laissons les curieux explorer la mine d'or qu'est cette base.

